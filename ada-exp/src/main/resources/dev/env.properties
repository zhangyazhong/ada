profile = ${profile}

source.hdfs.location.pattern = /home/hadoop/wiki/n_pagecounts-201601%02d-%02d0000

data.table.schema = wiki_ada
data.table.name = pagecounts
data.table.hdfs.location = /home/hadoop/spark/wiki_ada_pagecounts
data.table.structure = date_time int, project_name string, page_name string, page_count int, page_size int
data.table.terminated = ,

batch.table.name = pagecounts_batch
batch.table.hdfs.location = /home/hadoop/spark/wiki_ada_pagecounts_batch
batch.table.structure = date_time int, project_name string, page_name string, page_count int, page_size int
batch.table.terminated = ,

sample.table.schema = wiki_ada_verdict
sample.init.ratio = 10
sample.init.type = uniform,stratified
sample.init.stratified.column = project_name
sample.running.type = uniform,stratified

spark.executor.memory = 8g
spark.driver.memory = 12g
spark.sql.warehouse.dir = hdfs://master:9000/home/hadoop/spark/

exp.hour.init = 1
exp.hour.start = 24
exp.hour.total = 48
exp.hour.interval = 1