profile = ${profile}

source.hdfs.location.pattern = /zyz/wiki/n_pagecounts-201601%02d-%02d0000

data.table.schema = wiki_ada
data.table.name = pagecounts
data.table.hdfs.location = /zyz/spark/wiki_ada_pagecounts
data.table.structure = date_time int, project_name string, page_name string, page_count int, page_size int
data.table.terminated = ,

batch.table.name = pagecounts_batch
batch.table.hdfs.location = /zyz/spark/wiki_ada_pagecounts_batch
batch.table.structure = date_time int, project_name string, page_name string, page_count int, page_size int
batch.table.terminated = ,

sample.table.schema = wiki_ada_verdict
sample.init.ratio = 10
sample.init.type = uniform,stratified
sample.init.stratified.column = project_name
sample.running.type = uniform,stratified

spark.executor.memory = 64g
spark.driver.memory = 64g
spark.sql.warehouse.dir = hdfs://ubuntu1:9000/zyz/spark/

exp.hour.init = 0
exp.hour.start = 23
exp.hour.total = 48
exp.hour.interval = 1

database.restore = true