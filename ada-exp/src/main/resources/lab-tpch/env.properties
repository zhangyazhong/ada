profile = ${profile}

base.hdfs.location.pattern = /zyz/tpch/lineitem_base
source.hdfs.location.pattern = /zyz/tpch/lineitem_batch_%03d

data.table.schema = tpch
data.table.name = lineitem
data.table.hdfs.location = /zyz/spark/tpch_lineitem
data.table.structure = l_orderkey int, l_partkey int, l_suppkey int, l_linenumber int, l_quantity double, l_extendedprice double, \
                       l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate string, \
                       l_commitdate date, l_receipdate date, l_shipinstruct string, l_shipmode string, l_comment string
#data.table.structure = linenumber int, quantity double, extendedprice double, discount double, tax double, returnflag string
data.table.terminated = |

batch.table.name = lineitem_batch
batch.table.hdfs.location = /zyz/spark/tpch_lineitem_batch
batch.table.structure = l_orderkey int, l_partkey int, l_suppkey int, l_linenumber int, l_quantity double, l_extendedprice double, \
                       l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate string, \
                       l_commitdate date, l_receipdate date, l_shipinstruct string, l_shipmode string, l_comment string
batch.table.terminated = |

sample.table.schema = tpch_verdict
sample.init.ratio = 10
sample.init.type = uniform,stratified
sample.init.stratified.column = l_linenumber
sample.running.type = uniform,stratified

spark.executor.memory = 64g
spark.driver.memory = 64g
spark.sql.warehouse.dir = hdfs://ubuntu1:9000/zyz/spark/

exp.hour.init = 1
exp.hour.start = 0
exp.hour.total = 20
exp.hour.interval = 1