profile = ${profile}

source.hdfs.location.pattern = /zyz/tpch/lineitem_%d.csv

data.table.schema = tpch
data.table.name = lineitem
data.table.hdfs.location = /zyz/spark/tpch_lineitem
data.table.structure = orderkey int, partkey int, suppkey int, linenumber int, quantity double, extendedprice double, \
                       discount double, tax double, returnflag string, linestatus string, shipdate date, \
                       commitdate date, receipdate date, shipinstruct string, shipmode string, comment string
data.table.terminated = |

batch.table.name = lineitem_batch
batch.table.hdfs.location = /zyz/spark/tpch_lineitem_batch
batch.table.structure = orderkey int, partkey int, suppkey int, linenumber int, quantity double, extendedprice double, \
                        discount double, tax double, returnflag string, linestatus string, shipdate date, \
                        commitdate date, receipdate date, shipinstruct string, shipmode string, comment string
batch.table.terminated = |

sample.table.schema = tpch_verdict
sample.init.ratio = 10
sample.init.type = uniform,stratified
sample.init.stratified.column = project_name
sample.running.type = uniform,stratified

spark.executor.memory = 64g
spark.driver.memory = 64g
spark.sql.warehouse.dir = hdfs://ubuntu1:9000/zyz/spark/

exp.hour.init = 1
exp.hour.start = 24
exp.hour.total = 48
exp.hour.interval = 1